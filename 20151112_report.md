The primary activties in the past six months focused on

1 Exploring the effect of fiber assignment on the completeness and
clustering statistics for the Dark Time Survey (DTS).
2 Including target selection from observations and mocks into the
end-to-end data flow.
3 Including targets from the Bright Time Survey (BTS) into the
end-to-end data flow.
4. Defining new data structures, formats and performing the first
end-to-end tests.



1.Fiber Assignment
This required the detection of positioner collision detection into the
fiber assignment code to avoid them at the moment of
assignment. Additional subroutines have been added to increase the DTS
efficiency.
We explored the first strategies to mitigate fiber assignment effects.
In collaboration with Lado Samushia 
This exploration only took into account the effect of tiling and fiber
assignment. It did not include any inefficiency due to specstroscopic
variables.


2. Target Selection Data
We used DECaLS data to produce a target list and ran it through fiber
assignment. This test was mainly intended to test the database
interface vs. direct acces to files. At this point we tend to prefer a
direct access to files for the end-to-end data flow.
We also interfaced with the Durham group to have new mock data from
them into the formats we need. This had also the advantage to run
the fiber assignment on their computing systems and improve the code's
usability.

3. Bright Time Survey

We interfaced with Jeremy Tinker and Connie Rockosi to have mock
data from the Bright Galaxy Survey and the Milky Way Survey into
the required formats for the end-to-end dataflow. We also were able to
make the first test runs on their data.

4. Data structure

We have reviewed the data flow from target selection down to a large
scale structure catalog. From this exercise we have defined a few
standard formats and data structures to simulate this flow. We have
refactorized the fiber assignment code in order to comply to this
structure. Recently, we have been able to run the first tests of this
data structure on simplified end-to-end sims starting from mock data,
going through fiber assignment to produce a mock spectroscopic
catalog. 

In the next six months we will focus on

1. Interfacing with the group simulating spectroscopic success rates
in order to have more realistic spectrocopic catalogs at the end of
the data flow.

2. Establishing a system of prioriites and sub-priorities to accommodate both recorded randomization of the targets and specific priorities within certain samples, for example for the Milky Way Survey.

3. Integrating the Merged Taret List - Fiber Assignment - Spectroscopic Pipeline Simulation loop within a larger structure in order to measure performance on simulated target samples for both the dark time and bright time programs.
